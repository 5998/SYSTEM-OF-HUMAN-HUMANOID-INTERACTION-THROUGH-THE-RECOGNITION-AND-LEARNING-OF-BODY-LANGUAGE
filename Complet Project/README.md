# Recognition and learning body lenguage system.

### Description: 

The script developed [Recognition_And_Learning_Body_Lenguage.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Recognition_And_Learning_BodyLenguage_System.py) works using the GUI created **Recognition_And_Learning_Body_LenguagueGUI.ui** to perform the interaction with the [Robot Pepper](https://www.ald.softbankrobotics.com/en/robots/pepper). The idea of this system is that the humans can increase the confiance with the humanoids robots, throught the comprehension of the human's body lenguage by the robots, of this way the robots will interact in a coherent form with the human mood. This interaction is carried out using verbal and notverbal lenguage. 

The key of the system is the feature that allow of it understand the mood of the humans throught the body lenguage. This is possible implementing a [Neural network model](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Classify%20emotions) which was buiilt to classify the body lenguage in six categories, recognizing if the person is: Happy, Sad, Angry, Surprised, Relfexive or in a Normal state. The data used by the NNA is gotten using the **Microsoft Kinect V2 Camera** which allow compute the skeleton tracking and calcule the spatial ubication and the orientation of the body joints, with the orientation expressed like eulerian angles **(Yaw, Roll, Pitch)** from the head, shoulders, elbows, wrist, waist, hips and knees; and also the state of the hands **(close or open)** are gotten 23 values which are the input of the NNA.

After of the human mood is recognize, the system send a speech and a motion sequence (behavior animation), coherent with the emotion that the person is expresing with his body, to be spoken and played by the robot, of this way the robot interact with the humans having with base their mood. 

Also, the system allow to have conversation with the robot using the speech recognition system which catch the audio data and transform it into a text string. This string is used like the input a of [Long Short Term Memory network (LSTM)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), which is a special kind of [Recurrent neural networks (RNN)](http://www.felixgers.de/papers/phd.pdf), capable of learning long-term dependencies, used to generate answers to the user's speech and then to make that the robot spoken the answers. Complementing the message spoken, the robot also perform a motion sequence (behavior animation) with its body in coherent way with the speech. 

The system developed has three more functions, one of them allow to the user to play to be imitated by the robot. All the body moves that the user makes in front the Kinect camera, will be copied by the robot. Another function must to be taken as game in which the user perform a motion sequence and the robot reproduce a complet different motion sequence (generated by the [Generative Adversarial Network model](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Create%20Motion%20Sequences)) showing how will be the user's moves in an "Alternative world". Finally the system allow create new motion sequences (behavior animations), throught the imitation technique, to increase the motion sequences that could be reproduce by the robot. 

* The image is a graphical representation of the system described.
![recogniton and learning body lenguage system](https://user-images.githubusercontent.com/31509775/33132372-86f54870-cf67-11e7-8f8f-106fbcedbe5c.PNG)

### RNA models used:

The ANN built to perform the recognition of the emotions using the body posture data, is a two hidden layers network that has 23 inputs and 6 ouputs that generate a codification to each emotion class designated.

* The image is a representation of the neural network built.
![ann classification problem](https://user-images.githubusercontent.com/31509775/32928269-202f9f10-cb1f-11e7-8cfe-25d3a82e2511.PNG)

The LSTM were introduced by [Hochreiter & Schmidhuber (1997)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory), the LSTM, like all RNN, have the form of a chain of repeating modules of a neural network; but the difference with the regulars RNN is that this newtworks have reapeting modules with a very simple structure (a single tanh layer); and the LSTM has a reapting module with a different structure, in fact there are four neural network layers interacting in a very special way.

* The image illustrates the repeating module of a normal RNN and the LSTM network.
![lstm repeating module](https://user-images.githubusercontent.com/31509775/32632538-ffd338f6-c571-11e7-91e5-ac63bf978448.png)

The LSTM model used to introduce the conversation option was inspired in the [Neural conversation model](https://arxiv.org/pdf/1506.05869v3.pdf) which is caracterize by use one sequence to get another sequence, seq2seq, this model was built using the [Chatterbot wrapper](http://chatterbot.readthedocs.io/en/stable/index.html), disponible to python, which is a library that makes easier generate automates responses to string inputs using a selectrion of machine learning algorithms, including RNN and LSTM models, to produce different types of responses. 

* The image represents the LSTM model implemented with the Chatterbot tool.
![lstm](https://user-images.githubusercontent.com/31509775/32928662-1654f376-cb21-11e7-9464-0a1ebd1df5c9.png)

The GAN model used to created "originals" motion sequences, in fact is a joint work of two neural networks, the first of them create new data, using deconvolutional layers, from a random numerics value, and the second compare this data with the original, data presented by the user in the training, to decide which is real and which is fake; using convolutional layers. Epoch by epoch the first neural network goes increasing the perform of the data created with the feedback of the second network. 

* The image represents the first neural model structure used in the GAN model.
![generative model7](https://user-images.githubusercontent.com/31509775/32303654-e7969c1e-bf37-11e7-83f8-d0871afc6ae4.PNG)

* The image represents the second neural model structure used in the GAN model.
![discriminative model](https://user-images.githubusercontent.com/31509775/32345300-c775c43c-bfd7-11e7-824c-e1d53ca4f967.PNG)

### Hardware and software requirements:

* Microsoft Kinect V2 sensor.

![kinect](https://user-images.githubusercontent.com/31509775/32930198-222ed504-cb2b-11e7-8455-ba7d30df2631.jpg)

* Microsoft Kinect V2 adapter. 

![kinect adapter](https://user-images.githubusercontent.com/31509775/32930206-2a22a600-cb2b-11e7-86f9-96ecb8669ddc.jpg)

* Robot Pepper developed by Aldebaran robotics from SoftBank group. 

![robot peppper](https://user-images.githubusercontent.com/31509775/33133904-3c61430e-cf6c-11e7-9e78-a23be64623ab.png)

* Optional: GPU card with CUDA Compute Capability 3.0 or higher [(List of supported GPU cards in NVIDIA Documentation)](https://developer.nvidia.com/cuda-gpus).

![gpu](https://user-images.githubusercontent.com/31509775/32930230-5831bcfc-cb2b-11e7-8005-4cac20045a18.png)

This system was developed using **PYTHON 2.7 (32 bits) in WINDOWS 10** to run correctly this script is necessary first to have installed:

Is recommended install [Anaconda (Python 2.7 (32 bits) version](https://www.anaconda.com/download/#windows) to get easier the packages necessaries. 

* [Coregraphe suit version 2.5.5 from SoftBank Robotics Community](http://doc.aldebaran.com/2-5/software/choregraphe/installing.html#desktop-installation): Aldebaran documentation website. Choregraphe suit installation guide.
* [pynaoqi version 2.5.5.5](http://doc.aldebaran.com/2-5/dev/python/install_guide.html#python-install-guide): Aldebaran documentation website. Python SDK installation guide.
* [Kinect for Windows SDK version 2.0](https://www.microsoft.com/en-us/download/details.aspx?id=44561): Microsoft website link with the installation instructions.
* [pykinect2](https://github.com/Kinect/PyKinect2): GitHub link, this repository has all the instructions to use the Kinect V2 with Python.
* [PyQT4 GPL version 4.11.4 for Python 2.7 (32 bits)](https://sourceforge.net/projects/pyqt/files/PyQt4/PyQt-4.11.4/PyQt4-4.11.4-gpl-Py2.7-Qt4.8.7-x32.exe/download): Direct downlad link.
* [pygame version 1.9.2](http://www.pygame.org/news): pygame website link, you can found the download option for the pygame version 1.9.2 there.
* [keras version 2.0.6](https://keras.io/#installation): keras website link with all installation instructions.
* [Theano version 0.9.0 (keras backend engine)](http://deeplearning.net/software/theano/install_windows.html): theano windows installation instructions link.
* [speech recognition version 3.7.1](https://pypi.python.org/pypi/SpeechRecognition/): pypi.python website, all the installation instruction are specified there. (possible requirements: pyaudio version 0.2.11).
* [chatterbot version 0.7.6](http://chatterbot.readthedocs.io/en/stable/setup.html): chatterbot website installation instructions link.
* [cv2 version 3.0.0](https://docs.opencv.org/3.3.1/d5/de5/tutorial_py_setup_in_windows.html): OpenCV website link.
* numpy version 1.12.1.
* ctypes version 1.1.0

Optional software:

* [CUDAÂ® Toolkit 8.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/): Ensure that you append the relevant Cuda pathnames to the %PATH% environment variable as described in the NVIDIA documentation.
* The NVIDIA drivers associated with CUDA Toolkit 8.0.
* [cuDNN v6.1](https://developer.nvidia.com/cudnn): Note that cuDNN is typically installed in a different location from the other CUDA DLLs. Ensure that you add the directory where you installed the cuDNN DLL to your %PATH% environment variable.

### WARNINGS:

* To download the Aldebaran softeare and SDK is necessary to have a valid account and a register robot Pepper interface.

* Please make sure of download all the scripts and files presented in this repository.

* The line 38 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) allows load the ANN model built (you can re-train or modify this model with the script [RNA_Emotions_BodyPosture_Keras_Tensorflow.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Classify%20emotions)), the model is avaible in this repository with the name **"Model_RNA_Recognition_Of_Emotions"** in the folder [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models). **Please make sure that the path of the file is correct !**

```python
    37       # Is loaded the RNA model that perform the recognitions of emotions, using the joint angles information.  
    38       self.RNA_Emotions = load_model('...\Data_And_RNA_Models\Model_RNA_Recognition_Of_Emotions')
```

* The line 41 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) allows load the motion sequences data base used to make that Pepper reproduce behaviors animations (You can found the data base: **"Animations Sequences.csv"** in the folder: [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models). **Please make sure that the path of the file is correct !**

```python
    40       # Is loaded the Motion animations database.
    41       self.AnimationsDataBase = pd.read_csv('...\Data_And_RNA_Models\Animations Sequence.csv', header = 0)
    42       self.AnimationsDataBase.set_index("Animation", inplace = True)
```

* The lines 45-47 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) should import the GAN models built to create new "originals" motion sequences, to be used in the "Alternative world" function, but they are commented because the in the version 2.7 of pyhton is not avaible tensorflow like backend to keras, so is necessary use theano, and this doesnÂ´t the features to run the some layers of the models built. You can found the models in the in the folder [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models).

```python
    44       # Are loaded the GAN models, that perform the creation of Motion animation sequences.
    45       #self.Adversarial = load_model('...\Data_And_RNA_Models\Adversarial Model')
    46       #self.Discriminator = load_model('...\Data_And_RNA_Models\Discriminator Model')
    47       #self.Generator = load_model('...\Data_And_RNA_Models\Generator Model')
```

* Beacause the previous explanation, the lines 181-187 of the same Wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) are used to load the data base of the motion sequences created using the GAN model (you can found it in this repository in the folder: [Create motion sequences](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Create%20Motion%20Sequences) and is recommended first run this script to generate the motion sequences), selecting one of them each time in a random form. This is used to perform the Pepper's behavior in the "Alternative world" function. The logic to select and import the motion sequences follow the current explanation: is select a random epoch, 1 to 300, and then is selectec a random motion sequence, 1 to 32 (the author trained the GAN model by 300 epochs and save all the data generated, in each epoch are generated 32 motion sequences). Finally are loaded the file with the structure: "NewAnimation Epoch-Motion sequence"; example: "NewAnimation 100-32".

**Please, if you generate a different number of motion sequences with the GAN model, change the random and validations values.**
**Please make sure that the path to load the file is correct. The folder "DataBaseGeneratedByRNA" is in the folder: [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models). And has a few motion sequences developed; if you run the GAN model, it goes to save the motion sequences created in that folder, please check the path too.**

```python
   181      # Are get two random int to select a original animation of the DataSet.
   182      Iteration = np.random.randint(301); Iteration = [1 if Iteration == 0 else Iteration]; Iteration = [300 if Iteration == 301 else Iteration]
   183      AnimationData = np.random.randint(33); AnimationData = [1 if AnimationData == 0 else AnimationData]; AnimationData = [300 if AnimationData == 301 else AnimationData]
   184       
   185       # Is loaded a original animation of the DataSet.
   186       FileName = ("...\Data_And_RNA_Models\DataBaseGeneratedByRNA\ NewAnimation " + str(Iteration[0][0]) + "-" + str(AnimationData[0][0]) + ".csv")
   187       File = pd.read_csv(FileName, header = None)
```

* To create a new corpus data in the chatterbot library, is necessary go to the chatterbot_corpus folder and then into the data folder (installed if you use Anaconda in: **C:\...\Anaconda2_Win32\Lib\site-packages\chatterbot_corpus\data**) and create a new folder with the name "Pepper_Speech" (this was the name used by the author to create the folder in which was saved the corpus with the dialogues of the Conversation with Pepper) **Is important that if you select a different name, change the corpus name selector in the line 56 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) **), and then create a file with the name **"myown.yml"** (you can open this file with any text editor) and paste the dialogues: [Conversation with Pepper.txt](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Data_And_RNA_Models/Conversation%20with%20Pepper.txt).

```python
    49      # Is configured a ChatBot model that allow get answers to pre-define questions.         
    50      self. PepperSay = ChatBot('Pepper Answers', 
    51                                 logic_adapter = ["chatterbot.logic.MathematicalEvualation",
    52                                                  "chatterbot.logic.TimeLogicAdapter",
    53                                                  "chatterbot.logic.BestMatch"])
    54       
    55      self.PepperSay.set_trainer(ChatterBotCorpusTrainer)                     # Configure the train method.
    56      self.PepperSay.train("chatterbot.corpus.Pepper_Speech")                 # Train the model, with a specific dataset.
```

* To make that the robot Pepper perform a specific behavior coherent with its speech, is used a second LSTM model created with the chatterbot tool, which its input is the answer generated by the first one, and their output is the tag with the name of the motion sequence that the robot goes to reproduce. This model must to be configurated in the same form that the previous, creating a folder in the corpus folder with the name "Pepper_Speech_Animation" **(Remind if you select a different name, you have to check the name in the line 62 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38))**, and then create a file with the name **"myown.yml"** and paste the dialogues: [Tags to play motion sequences in conversations.txt](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Data_And_RNA_Models/Tags%20to%20play%20motion%20sequences%20in%20conversations.txt) 

```python
    58      # Is configured a ChatBot model that allow get the animation codes consistently with the speech.
    59      self.PepperAnimation = ChatBot('Pepper Speech Animations')
    61  
    62      self.PepperAnimation.set_trainer(ChatterBotCorpusTrainer)               # Configure the train method.
    63      self.PepperAnimation.train("chatterbot.corpus.Pepper_Speech_Animation") # Train the model, with a specific dataset.
```

* The lines 56 and 63 allow the training of the chatterbot models, when the training end, the file **"db.sqlite3"** is created, if you want, you can comment this line to avoid the time of the training, but please make shure that you have downloaded the **"db.sqlite3"** file into the folder: [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models).

* The wrapper: [Save_Animation.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Save_Animation.py#L224-L235) is used to save the new motion sequences created. **Please make sure that the path of the file to save the new .csv files is correct in the lines 229 and 240**. 

```python
   229      FileName = str("...\Data_And_RNA_Models\New_Animation_Data\New_Animation_" + str(FileNumber) + ".csv")
   230      File = pd.read_csv(FileName, header = None)
   ...
   240      FileName = str("...\Data_And_RNA_Models\New_Animation_Data\New_Animation_" + str(FileNumber) + ".csv")
   241      File = open(FileName, 'w') 
```

* The line 71 of the main script allows load the GUI developed to use the system. **Please make sure that the path of the file is correct !**

```python
    70          # Is "imported" the file that get the GUI.                
    71          self.MyGUI = uic.loadUi('...\Recognition_And_Learning_BodyLenguageGUI.ui', self)
```

### Code explanation:

The perform of the system is based in the use of the GUI developed; while the GUI be on, thw system goes to be working, allowing the interaction with the Robot Pepper throught two principal functions, the first one the recognition of the emotions to make that Pepper reproduce a coherent behavior, and the recognition of the speech that allow have conversation with Pepper in a question-answer way. Also, the system can be used throught three extra functions that just are avaible when the Microsoft Kinect V2 camera is working, this functions are: The imitation mode, the "Alvertanite world" game and the option to create and save new motion sequences. 

When the system start, the GUI is initiated and the first part is create the connection with the Robot Pepper, using a IP and Port Numbers. If the connection is successful, the user can activate the recognition of the emotions or the speech recognition function. Is this last is activated, in any momment, the Kinect camera will be disconnect, and the user should end the speech recognition before to connect the Kinect Camera and start the recognition of emotions. If the Kinect camera is connected, the user can change in any momment to another function like the imitation mode, the "Alternative world" game or the creation of a new motion sequence, deactivating the recognition of emotions but no disconnecting the Kinect. 

* The image shows the flowchart of the system general process.
![flowchart recognition and learning body lenguage](https://user-images.githubusercontent.com/31509775/33159259-4e998996-cfdf-11e7-878f-a921dd9d63ab.PNG) 

***"Talk with Pepper function" (Speech recognition system):***

When the function that allow have conversation with the robot is activated, the system turn on the microphone of the computer and access to it to catch the user speech (is possible to access to a external microphone), the audio data is transformed to text string using the SpeechRecognition library. The next step is generate an answer to the user speech, using the text string like input to a LSTM model, then this answer, first, is used to generate a "Tag" to import the specific motion sequences that allow to Pepper has a coherent behavior with the speech, and then is sent to the robot and the answer is spoken by it. After the robot has spoken the answer, it start to reproduce the motion sequences (this is because, although the robot has a function to speak and move at the same time, is limited to one change of value in its joints, and the motion sequence has 39 changes for each Pepper joints). 

* The image shows the flowchart of the speech recognition system that allow to have conversation with Pepper.
![flowchart recognition and learning body lenguage speech recognition](https://user-images.githubusercontent.com/31509775/33161063-1326d44e-cfef-11e7-8ddc-7b8834e6940c.PNG) 

***"Understanding the humans emotions" (Recognition of emotions system):***

To start the interaction with Pepper based in the human mood, is necessary connect the Kinect Camera, inmediatly the system start to capture images, when is recognize a body in the image is computed the skeleton tracking of that body, getting the data about the spatial position and the orientation, expressed in eulerian angles (Roll, Yaw, Pitch), of each body joint, With that information is drawn the skeleton representation on the user body in the image showed in the GUI, and also is recognized the emotion represented by the body posture using the ANN model pre-trained. Finally when the emotion is determinated, is sent to the robot a speech and then a motion sequence to make that Pepper speak and have a behaivor coherent with the emotion, also, in the GUI is showed the emotion recognized using emojies.

* The image shows the main flowchart of the emotions recognition system.
![flowchart recognition and learning body lenguage emotions recognition](https://user-images.githubusercontent.com/31509775/33177302-1d58960a-d030-11e7-81e4-1f2209673043.PNG)

***"Mimic style" (Imitation function):***



***"Wear behavior" (Alternative world function):***



***"New animations" (Create and save new motion sequence function):***



***System internal process***

The ***skeleton tracking*** process start to save, and then show in the GUI, the new color frame catching by the Kinect and searching for a body in it. The kinect can recognize until six bodies at the same time, so just the information of the nearest body is used. The kinect can compute the spatial points and the orientation of each bodi joint, so the next step is come back and save this information from the kinect and draw the skeleton representation on the user body in the image.

* The image shows the flowchart of the skeleton tracking process.
![flowchart recognition and learning body lenguage skelton tracking](https://user-images.githubusercontent.com/31509775/33178946-cdec9fde-d035-11e7-8a69-41f3abf96fde.PNG)

To ***show image*** gotten by the Kinect to the user using the GUI, is necessary create a surface to "paste" the image in it, and then create a RGB image from the surface. Is necessary modify the structure of the matrix resulting to generate a 3-D matrix with the RGB format and the size of the GUI frame. Finally must be created a pixel map from the RGB image to be "printed" in the GUI frame.

* The image shows the flowchart of the show image process.
![flowchart show image](https://user-images.githubusercontent.com/31509775/33179040-1d333cec-d036-11e7-8cf8-9278e62027f6.PNG)

To ***draw the skeleton representation*** on the user's body in the image, is necessary use the spatial coordinates of two adjacent joints, for example, to draw the head is necesary use the head and the neck spatial coordinates, to draw the right arm is necessary use the right shoulder and right elbow spatial coordinates. 

* The image shows the logical process to draw the skeleton tracking.
![flowchart draw body](https://user-images.githubusercontent.com/31509775/32955651-9d23ae84-cb84-11e7-91b7-1035102d79a4.PNG)

The process to ***draw the "bones"*** of the body is a simple process that verify if each joint have been tracked correctly or not, and use the spatial coordinates (x, y) from the first joint to start the bone and the coordiantes of the second joint to end the bone. Is the both joints were tracked correctly the bone will be drawn with a green color, if one of them was not tracked correctly the bone will be red.

* The image shows the flowchart to the process to draw the bones.
![flowchart draw bones](https://user-images.githubusercontent.com/31509775/32955857-29cdbd5c-cb85-11e7-830a-82ebf09194be.PNG)

To ***get the eulerian angles (Yaw, Roll, Pitch)*** is necessary verify if the respective joint have been tracked correctly, if this the case, is get back the quaternion that contain the orientation and then is calculate the eulerian angles from it; is the joint was not tracked correctly its eulerian angles are saved with the "none" value. Depending of the function that be activated, at the end is conformed an array that contains 23 angles, or just 16; If is activated one function to control directly the Robot Pepper body throught the imitation of the user moves will be just 16 values in the array; and if the function to recognize the emotions is activated will be 23 values, adding the angles values of the both hips and knees. The process begin with the computing of the waist angles to guarantee tha have been done the skeleton tracking, and then are computed all the necessaries angles of each joint following a logic process.

* The image shows the flowchart to the computing eulerian angles process.
![flowchart recognition and learning body lenguage get eulerian angles process](https://user-images.githubusercontent.com/31509775/33179980-2f944e82-d039-11e7-881e-61ceae421c15.PNG)

To ***make that Pepper change its joints orientations*** first is determinaned by the function that is working. If the imitation mode is activated, just one change in each Pepper joint will be done each time. With the anothers functions the Pepper's joints goes to change its orientation several times (39 times). The control of the body start sending the waist angles vales, and then the head angles values, next to be send the right arm angles vales and finally the left arms angels values. All angles values must to be verify before to be sent, because the robot has a limitations in its joints moves ranges.

* The image shows the flowchart to send the angles values to the robot Pepper joints.
![flowchart recognition and learning body lenguage send to pepper the angles values](https://user-images.githubusercontent.com/31509775/33181547-9d8fdde8-d03e-11e7-8a98-95cf2f5cab0e.PNG)

The process to ***verify the angles values*** before send it to the robot, is based in the restrictions of it (you can go furthere finding more information about Pepper's restrictions moves [in this documentation](http://doc.aldebaran.com/2-5/family/pepper_technical/joints_pep.html)). Each angle is verify to know is belongs to a specifig range, depending of the position to the adjacent joints the value of a specific joint can has a different move range. If the value is within the range limits is sent, if not, is aproximated to the nearest limit and then is sent.

* The image shows the flowchart to verify the angles values process.
![flowchart recognition and learning body lenguage verify angles values process](https://user-images.githubusercontent.com/31509775/33181874-c5afe0ce-d03f-11e7-99e3-d096b582df00.PNG)





