 Recognition and learning body lenguage system.

### Description: 

The script [Recognition_And_Learning_Body_Lenguage.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Recognition_And_Learning_BodyLenguage_System.py) use the GUI developed **Recognition_And_Learning_Body_LenguagueGUI.ui** to perform the interaction with the [Robot Pepper](https://www.ald.softbankrobotics.com/en/robots/pepper). The idea of this system is that the humans can increase the confiance with the humanoids robots, throught the comprehension of the human's body lenguage by the robots, of this way the robots will interact in a coherent form with the human mood. This interaction is carried out using verbal and notverbal lenguage. 

The key of the system is the feature that allow of it understand the mood of the humans throught the body lenguage. This is possible implementing a [Neural network model][Neural network model](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Classify%20emotions) which was buiilt to classify the body lenguage in six categories, recognizing if the person is: Happy, Sad, Angry, Surprised, Relfexive or in a Normal state. The data used by the NNA is gotten using the **Microsoft Kinect V2 Camera** which allow compute the skeleton tracking and calcule the spatial ubication and the orientation of the body joints, with the orientation expressed like eulerian angles **(Yaw, Roll, Pitch)** from the head, shoulders, elbows, wrist, waist, hips and knees; and also the state of the hands **(close or open)** are gotten 23 values which are the input of the NNA.

After of the human mood is recognize, the system send a speech and a motion sequence (behavior animation), coherent with the emotion that the person is expresing with his body, to be spoken and played by the robot, of this way the robot interact with the humans having with base their mood. 

Also, the system allow to have conversation with the robot using the speech recognition system which catch the audio data and transform it into a text string. This string is used like the input a of [Long Short Term Memory network (LSTM)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), which is a special kind of [Recurrent neural networks (RNN)](http://www.felixgers.de/papers/phd.pdf), capable of learning long-term dependencies, used to generate answers to the user's speech and then to make that the robot spoken the answers. Complementing the message spoken, the robot also perform a motion sequence (behavior animation) with its body in coherent way with the speech. 

The system developed has three more functions, one of them allow to the user play to be imitated by the robot. All the body moves that the user makes in front the Kinect camera will be copied by the robot. Another function must to be taken as game in which the user perform a motion sequence and the robot reproduce a complet different motion sequence (generated by the [Generative Adversarial Network model](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Create%20Motion%20Sequences)) showing how will be the user's moves in an "Alternative world". Finally the system allow create new motion sequences (behavior animations), throught the imitation technique, to increase the motion sequences that could be reproduce by the robot. 



