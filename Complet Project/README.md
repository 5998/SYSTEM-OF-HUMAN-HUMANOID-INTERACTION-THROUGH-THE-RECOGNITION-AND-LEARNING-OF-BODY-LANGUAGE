# Recognition and learning body language system.

### Description: 

The script developed [Recognition_And_Learning_Body_Language.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANOID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Recognition_And_Learning_BodyLanguage_System.py) works using the GUI created **Recognition_And_Learning_Body_LenguagueGUI.ui** to perform the interaction with the [Robot Pepper](https://www.ald.softbankrobotics.com/en/robots/pepper). The idea of this system is that the humans can increase the confidence with the humanoids robots, through the comprehension of the human's body language by the robots, of this way the robots will interact in a coherent form with the human mood. This interaction is carried out using verbal and nonverbal language. 

The key of the system is the feature that allow of it understand the mood of the humans through the body language. This is possible implementing a [Neural network model](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Classify%20emotions) which was built to classify the body language in six categories, recognizing if the person is: Happy, Sad, Angry, Surprised, Rlefexive or in a Normal state. The data used by the NNA is gotten using the **Microsoft Kinect V2 Camera** which allow compute the skeleton tracking and calculate the spatial ubication and the orientation of the body joints, with the orientation expressed like eulerian angles **(Yaw, Roll, Pitch)** from the head, shoulders, elbows, wrist, waist, hips and knees; and also the state of the hands **(close or open)** are gotten 23 values which are the input of the NNA.

After of the human mood is recognize, the system sends a speech and a motion sequence (behavior animation), coherent with the emotion that the person is expressing with his body, to be spoken and played by the robot, of this way the robot interacts with the humans having with base their mood. 

Also, the system allows to have conversation with the robot using the speech recognition system which catch the audio data and transform it into a text string. This string is used like the input a of [Long Short Term Memory network (LSTM)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/), which is a special kind of [Recurrent neural networks (RNN)](http://www.felixgers.de/papers/phd.pdf), capable of learning long-term dependencies, used to generate answers to the user's speech and then to make that the robot spoken the answers. Complementing the message spoken, the robot also performs a motion sequence (behavior animation) with its body in coherent way with the speech. 

The system developed has three more functions, one of them allow to the user to play to be imitated by the robot. All the body moves that the user makes in front the Kinect camera, will be copied by the robot. Another function must to be taken as game in which the user perform a motion sequence and the robot reproduce a complete different motion sequence (generated by the [Generative Adversarial Network model](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Create%20Motion%20Sequences)) showing how will be the user's moves in an "Alternative world". Finally the system allow create new motion sequences (behavior animations), through the imitation technique, to increase the motion sequences that could be reproduce by the robot. 

* The image is a graphical representation of the system described.
![recogniton and learning body lenguage system](https://user-images.githubusercontent.com/31509775/33132372-86f54870-cf67-11e7-8f8f-106fbcedbe5c.PNG)

### RNA models used:

The ANN built to perform the recognition of the emotions using the body posture data, is a two hidden layers network that has 23 inputs and 6 outputs that generate a codification to each emotion class designated.

* The image is a representation of the neural network built.
![ann classification problem](https://user-images.githubusercontent.com/31509775/32928269-202f9f10-cb1f-11e7-8cfe-25d3a82e2511.PNG)

The LSTM were introduced by [Hochreiter & Schmidhuber (1997)](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory), the LSTM, like all RNN, have the form of a chain of repeating modules of a neural network; but the difference with the regulars RNN is that this networks have repaeting modules with a very simple structure (a single tanh layer); and the LSTM has a repeating module with a different structure, in fact there are four neural network layers interacting in a very special way.

* The image illustrates the repeating module of a normal RNN and the LSTM network.
![lstm repeating module](https://user-images.githubusercontent.com/31509775/32632538-ffd338f6-c571-11e7-91e5-ac63bf978448.png)

The LSTM model used to introduce the conversation option was inspired in the [Neural conversation model](https://arxiv.org/pdf/1506.05869v3.pdf) which is characterize by use one sequence to get another sequence, seq2seq, this model was built using the [Chatterbot wrapper](http://chatterbot.readthedocs.io/en/stable/index.html), available to python, which is a library that makes easier generate automates responses to string inputs using a selection of machine learning algorithms, including RNN and LSTM models, to produce different types of responses. 

* The image represents the LSTM model implemented with the Chatterbot toola
![lstm](https://user-images.githubusercontent.com/31509775/32928662-1654f376-cb21-11e7-9464-0a1ebd1df5c9.png)

The GAN model used to create "originals" motion sequences, in fact is a joint work of two neural networks, the first of them create new data, using deconvolutional layers, from a random numeric value, and the second compare this data with the original, data presented by the user in the training, to decide which is real and which is fake; using convolutional layers. Epoch by epoch the first neural network goes increasing the perform of the data created with the feedback of the second network. 

* The image represents the first neural model structure used in the GAN model.
![generative model7](https://user-images.githubusercontent.com/31509775/32303654-e7969c1e-bf37-11e7-83f8-d0871afc6ae4.PNG)

* The image represents the second neural model structure used in the GAN model.
![discriminative model](https://user-images.githubusercontent.com/31509775/32345300-c775c43c-bfd7-11e7-824c-e1d53ca4f967.PNG)

### Hardware and software requirements:

* Microsoft Kinect V2 sensor.

![kinect](https://user-images.githubusercontent.com/31509775/32930198-222ed504-cb2b-11e7-8455-ba7d30df2631.jpg)

* Microsoft Kinect V2 adapter. 

![kinect adapter](https://user-images.githubusercontent.com/31509775/32930206-2a22a600-cb2b-11e7-86f9-96ecb8669ddc.jpg)

* Robot Pepper developed by Aldebaran robotics from SoftBank group. 

![robot peppper](https://user-images.githubusercontent.com/31509775/33133904-3c61430e-cf6c-11e7-9e78-a23be64623ab.png)

* Optional: GPU card with CUDA Compute Capability 3.0 or higher [(List of supported GPU cards in NVIDIA Documentation)](https://developer.nvidia.com/cuda-gpus).

![gpu](https://user-images.githubusercontent.com/31509775/32930230-5831bcfc-cb2b-11e7-8005-4cac20045a18.png)

This system was developed using **PYTHON 2.7 (32 bits) in WINDOWS 10** to run correctly this script is necessary first to have installed:

Is recommended install [Anaconda (Python 2.7 (32 bits) version](https://www.anaconda.com/download/#windows) to get easier the packages necessaries. 

* [Coregraphe suit version 2.5.5 from SoftBank Robotics Community](http://doc.aldebaran.com/2-5/software/choregraphe/installing.html#desktop-installation): Aldebaran documentation website. Choregraphe suit installation guide.
* [pynaoqi version 2.5.5.5](http://doc.aldebaran.com/2-5/dev/python/install_guide.html#python-install-guide): Aldebaran documentation website. Python SDK installation guide.
* [Kinect for Windows SDK version 2.0](https://www.microsoft.com/en-us/download/details.aspx?id=44561): Microsoft website link with the installation instructions.
* [pykinect2](https://github.com/Kinect/PyKinect2): GitHub link, this repository has all the instructions to use the Kinect V2 with Python.
* [PyQT4 GPL version 4.11.4 for Python 2.7 (32 bits)](https://sourceforge.net/projects/pyqt/files/PyQt4/PyQt-4.11.4/PyQt4-4.11.4-gpl-Py2.7-Qt4.8.7-x32.exe/download): Direct download link.
* [pygame version 1.9.2](http://www.pygame.org/news): pygame website link, you can found the download option for the pygame version 1.9.2 there.
* [keras version 2.0.6](https://keras.io/#installation): keras website link with all installation instructions.
* [Theano version 0.9.0 (keras backend engine)](http://deeplearning.net/software/theano/install_windows.html): theano windows installation instructions link.
* [speech recognition version 3.7.1](https://pypi.python.org/pypi/SpeechRecognition/): pypi.python website, all the installation instruction are specified there. (possible requirements: pyaudio version 0.2.11).
* [chatterbot version 0.7.6](http://chatterbot.readthedocs.io/en/stable/setup.html): chatterbot website installation instructions link.
* [cv2 version 3.0.0](https://docs.opencv.org/3.3.1/d5/de5/tutorial_py_setup_in_windows.html): OpenCV website link.
* numpy version 1.12.1.
* ctypes version 1.1.0

Optional software:

* [CUDAÂ® Toolkit 8.0](http://docs.nvidia.com/cuda/cuda-installation-guide-microsoft-windows/): Ensure that you append the relevant Cuda pathnames to the %PATH% environment variable as described in the NVIDIA documentation.
* The NVIDIA drivers associated with CUDA Toolkit 8.0.
* [cuDNN v6.1](https://developer.nvidia.com/cudnn): Note that cuDNN is typically installed in a different location from the other CUDA DLLs. Ensure that you add the directory where you installed the cuDNN DLL to your %PATH% environment variable.

### WARNINGS:

* To download the Aldebaran software and SDK is necessary to have a valid account and a register robot Pepper interface.

* Please make sure of download all the scripts and files presented in this repository.

* The line 38 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) allows load the ANN model built (you can re-train or modify this model with the script [RNA_Emotions_BodyPosture_Keras_Tensorflow.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Classify%20emotions)), the model is available in this repository with the name **"Model_RNA_Recognition_Of_Emotions"** in the folder [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models). **Please make sure that the path of the file is correct !**

```python
    37       # Is loaded the RNA model that perform the recognitions of emotions, using the joint angles information.  
    38       self.RNA_Emotions = load_model('...\Data_And_RNA_Models\Model_RNA_Recognition_Of_Emotions')
```

* The line 41 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) allows load the motion sequences data base used to make that Pepper reproduce behaviors animations (You can found the data base: **"Animations Sequences.csv"** in the folder: [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models). **Please make sure that the path of the file is correct !**

```python
    40       # Is loaded the Motion animations database.
    41       self.AnimationsDataBase = pd.read_csv('...\Data_And_RNA_Models\Animations Sequence.csv', header = 0)
    42       self.AnimationsDataBase.set_index("Animation", inplace = True)
```

* The lines 176-178 of the same Wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) are used to load the data base of the motion sequences created using the GAN model (you can found it in this repository in the folder: [Create motion sequences](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Nueral%20Networks/Create%20Motion%20Sequences) and is recommended first run this script to generate the motion sequences), selecting one of them each time in a random form. This is used to perform the Pepper's behavior in the "Alternative world" function. The logic to select and import the motion sequences follow the current explanation: is select a random epoch, 1 to 300, and then is selected a random motion sequence, 1 to 64 (the author selected the data of the last 300 epochs of the GAN model and save all the data generated). Finally are loaded the file with the structure: "NewAnimation Epoch-Motion sequence"; example: "NewAnimation 100-32".

**Please, if you generate a different number of motion sequences with the GAN model, change the random and validations values.**
**Please make sure that the path to load the file is correct. The folder "DataBaseGeneratedByRNA" is in the folder: [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models). And has a few motion sequences developed; if you run the GAN model, it goes to save the motion sequences created in that folder, please check the path too.**

```python
   176      # Are get two random int to select a original animation of the DataSet.
   177      Iteration = np.random.randint(301); Iteration = [1 if Iteration == 0 else Iteration]; Iteration = [300 if Iteration == 301 else Iteration]
   178      AnimationData = np.random.randint(65); AnimationData = [1 if AnimationData == 0 else AnimationData]; AnimationData = [64 if AnimationData == 65 else AnimationData]
   179       
   180       # Is loaded a original animation of the DataSet.
   181       FileName = ("...\Data_And_RNA_Models\DataBaseGeneratedByRNA\ NewAnimation " + str(Iteration[0][0]) + "-" + str(AnimationData[0][0]) + ".csv")
   182       File = pd.read_csv(FileName, header = None)
```

* To create a new corpus data in the chatterbot library, is necessary go to the chatterbot_corpus folder and then into the data folder (installed if you use Anaconda in: **C:\...\Anaconda2_Win32\Lib\site-packages\chatterbot_corpus\data**) and create a new folder with the name "Pepper_Speech" (this was the name used by the author to create the folder in which was saved the corpus with the dialogues of the Conversation with Pepper) **Is important that if you select a different name, change the corpus name selector in the line 56 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38) **), and then create a file with the name **"myown.yml"** (you can open this file with any text editor) and paste the dialogues: [Conversation with Pepper.txt](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Data_And_RNA_Models/Conversation%20with%20Pepper.txt).

```python
    49      # Is configured a ChatBot model that allow get answers to pre-define questions.         
    50      self. PepperSay = ChatBot('Pepper Answers', 
    51                                 logic_adapter = ["chatterbot.logic.MathematicalEvualation",
    52                                                  "chatterbot.logic.TimeLogicAdapter",
    53                                                  "chatterbot.logic.BestMatch"])
    54       
    55      self.PepperSay.set_trainer(ChatterBotCorpusTrainer)                     # Configure the train method.
    56      self.PepperSay.train("chatterbot.corpus.Pepper_Speech")                 # Train the model, with a specific dataset.
```

* To make that the robot Pepper perform a specific behavior coherent with its speech, is used a second LSTM model created with the chatterbot tool, which its input is the answer generated by the first one, and their output is the tag with the name of the motion sequence that the robot goes to reproduce. This model must to be configurated in the same form that the previous, creating a folder in the corpus folder with the name "Pepper_Speech_Animation" **(Remind if you select a different name, you have to check the name in the line 62 of the wrapper: [Neural_Networks_Models.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Neural_Networks_Models.py#L37-L38))**, and then create a file with the name **"myown.yml"** and paste the dialogues: [Tags to play motion sequences in conversations.txt](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Data_And_RNA_Models/Tags%20to%20play%20motion%20sequences%20in%20conversations.txt) 

```python
    58      # Is configured a ChatBot model that allow get the animation codes consistently with the speech.
    59      self.PepperAnimation = ChatBot('Pepper Speech Animations')
    61  
    62      self.PepperAnimation.set_trainer(ChatterBotCorpusTrainer)               # Configure the train method.
    63      self.PepperAnimation.train("chatterbot.corpus.Pepper_Speech_Animation") # Train the model, with a specific dataset.
```

* The lines 56 and 63 allow the training of the chatterbot models, when the training end, the file **"db.sqlite3"** is created, if you want, you can comment this line to avoid the time of the training, but please make sure that you have downloaded the **"db.sqlite3"** file into the folder: [Data_And_RNA_Models](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/tree/master/Complet%20Project/Data_And_RNA_Models).

* The wrapper: [Save_Animation.py](https://github.com/Ing-Mk-FranJa07/SYSTEM-OF-HUMAN-HUMANID-INTERACTION-THROUGH-THE-RECOGNITION-AND-LEARNING-OF-BODY-LANGUAGE/blob/master/Complet%20Project/Wrappers/Save_Animation.py#L224-L235) is used to save the new motion sequences created. **Please make sure that the path of the file to save the new .csv files is correct in the lines 229 and 240**. 

```python
   229      FileName = str("...\Data_And_RNA_Models\New_Animation_Data\New_Animation_" + str(FileNumber) + ".csv")
   230      File = pd.read_csv(FileName, header = None)
   ...
   240      FileName = str("...\Data_And_RNA_Models\New_Animation_Data\New_Animation_" + str(FileNumber) + ".csv")
   241      File = open(FileName, 'w') 
```

* The line 71 of the main script allows load the GUI developed to use the system. **Please make sure that the path of the file is correct !**

```python
    70          # Is "imported" the file that get the GUI.                
    71          self.MyGUI = uic.loadUi('...\Recognition_And_Learning_BodyLenguageGUI.ui', self)
```

### Code explanation:

The perform of the system is based in the use of the GUI developed; while the GUI be on, the system goes to be working, allowing the interaction with the Robot Pepper through two principal functions, the first one the recognition of the emotions to make that Pepper reproduce a coherent behavior, and the recognition of the speech that allow have conversation with Pepper in a question-answer way. Also, the system can be used through three extra functions that just are available when the Microsoft Kinect V2 camera is working, these functions are: The imitation mode, the "Alternative world" game and the option to create and save new motion sequences. 

When the system starts, the GUI is initiated and the first part is create the connection with the Robot Pepper, using a IP and Port Numbers. If the connection is successful, the user can activate the recognition of the emotions or the speech recognition function. Is this last is activated, in any moment, the Kinect camera will be disconnect, and the user should end the speech recognition before to connect the Kinect Camera and start the recognition of emotions. If the Kinect camera is connected, the user can change in any moment to another function like the imitation mode, the "Alternative world" game or the creation of a new motion sequence, deactivating the recognition of emotions but no disconnecting the Kinect. 

* The image shows the flowchart of the system general process.
![flowchart recognition and learning body lenguage](https://user-images.githubusercontent.com/31509775/33159259-4e998996-cfdf-11e7-878f-a921dd9d63ab.PNG) 

***"Talk with Pepper function" (Speech recognition system):***

When the function that allow have conversation with the robot is activated, the system turn on the microphone of the computer and access to it to catch the user speech (is possible to access to a external microphone), the audio data is transformed to text string using the SpeechRecognition library. The next step is yo generate an answer to the user speech, using the text string like input to a LSTM model, then this answer, first, is used to generate a "Tag" to import the specific motion sequences that allow to Pepper has a coherent behavior with the speech, and then is sent to the robot and the answer is spoken by it. After the robot has spoken the answer, it starts to reproduce the motion sequences (this is because, although the robot has a function to speak and move at the same time, is limited to one change of value in its joints, and the motion sequence has 39 changes for each Pepper joints). 

* The image shows the flowchart of the speech recognition system that allow to have conversation with Pepper.
![flowchart recognition and learning body lenguage speech recognition](https://user-images.githubusercontent.com/31509775/33161063-1326d44e-cfef-11e7-8ddc-7b8834e6940c.PNG) 

***"Understanding the humans emotions" (Recognition of emotions system):***

To start the interaction with Pepper based in the human mood, is necessary connect the Kinect Camera, immediately the system start to capture images, when is recognize a body in the image is computed the skeleton tracking of that body, getting the data about the spatial position and the orientation, expressed in eulerian angles (Roll, Yaw, Pitch), of each body joint, With that information is drawn the skeleton representation on the user body in the image showed in the GUI, and also is recognized the emotion represented by the body posture using the ANN model pre-trained. Finally when the emotion is determinate, is sent to the robot a speech and then a motion sequence to make that Pepper speak and have a behavior coherent with the emotion, also, in the GUI is showed the emotion recognized using emojies.

* The image shows the general flowchart of the emotions recognition system.
![flowchart recognition and learning body lenguage emotions recognition](https://user-images.githubusercontent.com/31509775/33219860-c149a4d8-d112-11e7-8d82-c4e86b93e07a.PNG)

***"Mimic style" (Imitation function):***

When the imitation function is activated, all the moves that the user makes with his body will copied by the robot in real time. The process to allow the imitation start to catching the images and show it in the GUI with the skeleton tracking of the body and compute the eulerian body joints angles to send them to the robot, if the tracking was correct. 

* The images shows the general flowchart of the imitation function.
![flowchart recognition and learning body lenguage imitation function](https://user-images.githubusercontent.com/31509775/33183858-a3dc2036-d047-11e7-8392-739f3b498c02.PNG)

***"Wear behavior" (Alternative world function):***

When the game "Alternative world" is activated, the user will have a few seconds to make a motion sequence with his body and then Pepper goes to reproduce a total different motion sequence. The process starts getting the image and show it in the GUI with the skeleton tracking (the original idea was used the tracking information to create a original motion sequences but because software limitations the real working doesnÂ´t used the tracking information), then is send to Pepper, a random motion sequence created by the GAN model previously.

* The image shows the general flowchart of the game "Alternative world".
![flowchart recognition and learning body lenguage alternative world](https://user-images.githubusercontent.com/31509775/33184023-9a2fc8a2-d048-11e7-8a6b-161cd81b922e.PNG)

***"New animations" (Create and save new motion sequence function):***

To create and save a new motion sequence, is given to the user 5 seconds to perform moves with his body while that is getting the images and the skeleton tracking to generate a visual feedback in the GUI and also is computed the skeleton information to create a 39 values by each one of the 15 angles (16 with the head Yaw that always is cero because the complexity to calculate it), if the tracking was correct and was possible to get the enough information, the next step is make that the robot reproduce the motion sequence to decide if this is acceptable, the user can saved creating a .csv file or deleted the motion sequences just clearing the information saved. If the tracking information was not enough is restarted the counter to start again.

* The image shows the general flowchart to the process that allow create a new motion sequence.
![flowchart recognition and learning body lenguage create new animations](https://user-images.githubusercontent.com/31509775/33184274-ba0c83ee-d049-11e7-9576-4c8997abc20a.PNG)

***System internal process***

The ***skeleton tracking*** process start to save, and then show in the GUI, the new color frame catching by the Kinect and searching for a body in it. The kinect can recognize until six bodies at the same time, so just the information of the nearest body is used. The kinect can compute the spatial points and the orientation of each body joint, so the next step is come back and save this information from the kinect and draw the skeleton representation on the user body in the image.

* The image shows the flowchart of the skeleton tracking process.
![flowchart recognition and learning body lenguage skelton tracking](https://user-images.githubusercontent.com/31509775/33178946-cdec9fde-d035-11e7-8a69-41f3abf96fde.PNG)

To ***show image*** gotten by the Kinect to the user using the GUI, is necessary create a surface to "paste" the image in it, and then create a RGB image from the surface. Is necessary modify the structure of the matrix resulting to generate a 3-D matrix with the RGB format and the size of the GUI frame. Finally must be created a pixel map from the RGB image to be "printed" in the GUI frame.

* The image shows the flowchart of the show image process.
![flowchart show image](https://user-images.githubusercontent.com/31509775/33179040-1d333cec-d036-11e7-8cf8-9278e62027f6.PNG)

To ***draw the skeleton representation*** on the user's body in the image, is necessary use the spatial coordinates of two adjacent joints, for example, to draw the head is necessary use the head and the neck spatial coordinates, to draw the right arm is necessary use the right shoulder and right elbow spatial coordinates. 

* The image shows the logical process to draw the skeleton tracking.
![flowchart draw body](https://user-images.githubusercontent.com/31509775/32955651-9d23ae84-cb84-11e7-91b7-1035102d79a4.PNG)

The process to ***draw the "bones"*** of the body is a simple process that verify if each joint have been tracked correctly or not, and use the spatial coordinates (x, y) from the first joint to start the bone and the coordinates of the second joint to end the bone. Is the both joints were tracked correctly the bone will be drawn with a green color, if one of them was not tracked correctly the bone will be red.

* The image shows the flowchart to the process to draw the bones.
![flowchart draw bones](https://user-images.githubusercontent.com/31509775/32955857-29cdbd5c-cb85-11e7-830a-82ebf09194be.PNG)

To ***get the eulerian angles (Yaw, Roll, Pitch)*** is necessary verify if the respective joint have been tracked correctly, if this the case, is get back the quaternion that contain the orientation and then is calculate the eulerian angles from it; is the joint was not tracked correctly its eulerian angles are saved with the "none" value. Depending of the function that be activated, at the end is conformed an array that contains 23 angles, or just 16; If is activated one function to control directly the Robot Pepper body through the imitation of the user moves will be just 16 values in the array; and if the function to recognize the emotions is activated will be 23 values, adding the angles values of the both hips and knees. The process begins with the computing of the waist angles to guarantee that have been done the skeleton tracking, and then are computed all the necessaries angles of each joint following a logic process.

* The image shows the flowchart to the computing eulerian angles process.
![flowchart recognition and learning body lenguage get eulerian angles process](https://user-images.githubusercontent.com/31509775/33179980-2f944e82-d039-11e7-881e-61ceae421c15.PNG)

To ***make that Pepper change its joints orientations*** first is determined by the function that is working. If the imitation mode is activated, just one change in each Pepper joints will be done each time. With the another functions the Pepper's joints goes to change its orientation several times (39 times). The control of the body start sending the waist angles values, and then the head angles values, next is sent the right arm angles values and finally the left arms angles values. All angles values must to be verify before to be sent, because the robot has limitations in its joints moves ranges.

* The image shows the flowchart to send the angles values to the robot Pepper joints.
![flowchart recognition and learning body lenguage send to pepper the angles values](https://user-images.githubusercontent.com/31509775/33181547-9d8fdde8-d03e-11e7-8a98-95cf2f5cab0e.PNG)

The process to ***verify the angles values*** before send it to the robot, is based in the restrictions of it (you can go further finding more information about Pepper restrictions of moves [in this documentation](http://doc.aldebaran.com/2-5/family/pepper_technical/joints_pep.html)). Each angle is verified to know is belongs to a specific range, depending of the position to the adjacent joints, the value of a specific joints can has a different move range. If the value is within the range limits,  is sent, if not, is approximated to the nearest limit and then is sent.

* The image shows the flowchart to verify the angles values process.
![flowchart recognition and learning body lenguage verify angles values process](https://user-images.githubusercontent.com/31509775/33181874-c5afe0ce-d03f-11e7-99e3-d096b582df00.PNG)

The last internal process is used just to the function that allow create and save new motion sequences and this process is used to ***verify and change the "None" angles values***, if is found a "none" value, this is replaced by the previous value. This process is done by all joints values.

* The image shows the flowchart to verify the "None" values process.
![flowchart recognition and learning body lenguage verify none values](https://user-images.githubusercontent.com/31509775/33183606-7c7feece-d046-11e7-9bcf-e54c3173feed.PNG)

### System user guide:

* The image shows the GUI developed to use the system:
![recognition and learning body lenguage gui](https://user-images.githubusercontent.com/31509775/33232820-ced54c92-d1da-11e7-8176-625e29ec371c.PNG)

When the system is started the GUI appear. The first step to use the system is do the connection with the robot Pepper, or the virtual robot that you can use with the Choregraphe software. You need the Ip number (127.0.0.1 in the majorie of the cases) and the Port number (you can find the port number of the virtual robot in the Choregraphe suit going to Edit -> Preferences -> Virtual Robot). Now you can use the speech recognition system to have conversation with Pepper, activating the checkbox "Talk with Pepper deactivated" in that moment the text goes to change to "Talk with Pepper activated", deschecking the checkbox the function ends, and you can use the emotions recognition connecting the kinect camera. Also you can use the imitation mode or the alternative world game in any moment checking the checkbox in any moment. Is you want create a new motion sequence you can do it in any moment clicking on the button "Create new animation motion sequence" while the kinect be connected. 

* The image shows how start the system and use the different functions of them.
![recogniton and learning body lenguage user guide gif](https://user-images.githubusercontent.com/31509775/33186903-7f745128-d05b-11e7-8e2f-f0327d28e09c.gif)

In any moment the user can activate the speech recognition function, when this function is activated the kinect camera is disconnect, this happen because the actualization of the GUI take more time that the time that need the code to be ran, so the GUI canÂ´t show the new image, the change in the emojies and the speech and answer generated at the same time. Click again on the checkbox to deactivate this function and use the kinect camera again. The speech recognition takes a time while transform the speech to text. The GUI donÂ´t show the informative messages, so if you can open a console to follow the changes it will do better the perform of the function. Also, is recommended have a good microphone and be in a low noise environment. The chatterbot model can generate answer to similar sentences to that were trained, is not necessary that be the same sentences.

* The image shows a few sentences spoken to the system and the answer spoken by the robot before reproduce a motion sequence animation.

![speech recognition system](https://user-images.githubusercontent.com/31509775/33188139-44f4337e-d065-11e7-8fc0-fd1ec143ad93.gif)

The recognition of emotions function works using the ANN built and trained by the author and generate a speech and behaviors motion sequences created by him too. The advice is that you create your own data base with different persons trying to represent the emotions with theirs bodies to re-train the neural model and increase the performance of the system. 

* The image shows some examples of the response of the system using the robot Pepper like interface front the mood expressed by the user with his posture.

![recognition of emotions sytem gif](https://user-images.githubusercontent.com/31509775/33188045-7de6951a-d064-11e7-8b92-791e4e591f54.gif)

The imitation function allows the robot replay the moves performed by the user in front the kinect. Is necessary that the tracking be completed to send the robot the angles values. The robot has limitations so, there are some many moves that it can't replay.

* The image shows the imitation function working.

![imitation function gif](https://user-images.githubusercontent.com/31509775/33188193-d6f4c176-d065-11e7-841a-6b2c3d0a5334.gif)

The alternative world is game in which the user has a few seconds to perform moves with his body and then the robot goes to reproduce that moves like if they would be perform in an alternative world. 

* The image shows an example of the alternative world game.

![alternative world game gif](https://user-images.githubusercontent.com/31509775/33188209-1400b17e-d066-11e7-9523-6c086c0191bc.gif)

The user has the option to create his own motion sequences to be reproduced by the robot. If the user wants, he goes to have 5 seconds to create with his body the motion sequence, then the robot goes to reproduce it, and is the user want, he can save it creating a .csv file with the data.

* The image shows an example of the option to create and save a new motion sequence.

![create a new motion sequence gif](https://user-images.githubusercontent.com/31509775/33188307-e3b09cc2-d066-11e7-95bd-0fb57996cc33.gif)

**Click on the images to see them with a better quality**
